{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleDigitMNISTNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SingleDigitMNISTNet, self).__init__()\n",
    "        # input MNIST images for nof_digits digit-image: 1 x nof_digitsx28 x nof_digitsx28\n",
    "        self.numChannels1 = 8\n",
    "        self.numChannels2 = 32\n",
    "        self.nof_classes = 10 # figures 0...9 (and \"not recognized\"? - is not in the trainingset/-labels)\n",
    "        self.conv1 = torch.nn.Conv2d(1, self.numChannels1, 5, padding=2, bias=False) # <- out: 8 x 28 x 28  # <- max-pooling out: 8 x 14 x 14\n",
    "        self.conv1_batchnorm = torch.nn.BatchNorm2d(num_features = self.numChannels1)\n",
    "        \n",
    "        # use normal initialization for conv1:\n",
    "        torch.nn.init.normal_(self.conv1.weight)\n",
    "        torch.nn.init.constant_(self.conv1_batchnorm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.conv1_batchnorm.bias)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(self.numChannels1, self.numChannels2, 3, padding=1, bias=False) #<- out: 32 x 14 x 14\n",
    "        self.conv2_batchnorm = torch.nn.BatchNorm2d(num_features = self.numChannels2)\n",
    "\n",
    "         # use normal initialization for conv2:\n",
    "        torch.nn.init.normal_(self.conv2.weight)\n",
    "        torch.nn.init.constant_(self.conv2_batchnorm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.conv2_batchnorm.bias)\n",
    "\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(self.numChannels2 *7 * 7, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, self.nof_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.conv1_batchnorm(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(x), (2,2))\n",
    "        x = self.conv2_batchnorm(self.conv2(x))\n",
    "        x = F.max_pool2d(F.relu(x), (2,2))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(x, dim=1) # use log_softmax() (i.e. log(softmax()) ) to use NLLLoss() as loss-function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo_img_batch.shape: torch.Size([2, 1, 28, 28])\n",
      "model output shape: torch.Size([2, 10])\n",
      "sum is equal (1,1)? : tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# check the network-definition:\n",
    "\n",
    "pseudo_img_batch = torch.rand(2,1,28,28)\n",
    "print(f\"pseudo_img_batch.shape: {pseudo_img_batch.shape}\")\n",
    "\n",
    "tmodel = SingleDigitMNISTNet()\n",
    "toutput = tmodel(pseudo_img_batch)\n",
    "print(f\"model output shape: {toutput.shape}\")\n",
    "print(f\"sum is equal (1,1)? : {toutput.exp().sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training(epochs, train_loader, model, loss_fn, optimizer, device, show_progress= False, L2_regularization=False, L1_regularization=False, L2_lambda=0.001, L1_lambda=0.001):\n",
    "    l2_norm = 0\n",
    "    l1_norm = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, y in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            yp = model(imgs)\n",
    "            loss = loss_fn(yp, y)\n",
    "            \n",
    "            if(L2_regularization):\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                loss = loss + L2_lambda * l2_norm\n",
    "            \n",
    "            if(L1_regularization):\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + L1_lambda * l1_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if(epoch == 1 or epoch%5 == 0):\n",
    "            print(F\"len train loader: {len(train_loader)}\")\n",
    "            print(f\"{datetime.datetime.now()} Epoch {epoch} Training loss {loss_train/ len(train_loader)}\")\n",
    "            #if(show_progress): # prints out some weights to see if anything happens at all:\n",
    "            #    print(model.conv1.weight[0][0:10])\n",
    "\n",
    "def validate(model, train_loader, val_loader, loss_fn):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        equals = 0\n",
    "        nof_y = 0\n",
    "        for imgs, y  in loader:\n",
    "            with torch.no_grad():\n",
    "                yp = model(imgs)\n",
    "                y_class = torch.argmax(yp, dim=1)\n",
    "                #print(f\"y.shape: {y.shape}\")\n",
    "                #print(f\"y_class.shape: {y_class.shape}\")\n",
    "                equals += torch.eq(y_class, y).sum()\n",
    "                nof_y += len(y)\n",
    "        \n",
    "        print(f\"Accuracy {name}: {equals/nof_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "len train loader: 1875\n",
      "2022-02-03 12:13:39.729588 Epoch 1 Training loss 1.1522051845868428\n",
      "len train loader: 1875\n",
      "2022-02-03 12:17:06.799479 Epoch 5 Training loss 0.44139853501319887\n",
      "len train loader: 1875\n",
      "2022-02-03 12:21:12.044140 Epoch 10 Training loss 0.4266221227486928\n",
      "len train loader: 1875\n",
      "2022-02-03 12:24:57.868195 Epoch 15 Training loss 0.42769466034173964\n",
      "len train loader: 1875\n",
      "2022-02-03 12:28:39.512479 Epoch 20 Training loss 0.43093685474395754\n",
      "len train loader: 1875\n",
      "2022-02-03 12:32:19.382802 Epoch 25 Training loss 0.4280794123093287\n",
      "len train loader: 1875\n",
      "2022-02-03 12:36:25.587965 Epoch 30 Training loss 0.43277747872273126\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# training:\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "# load datasets and create dataloader for BATCH_SIZE\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# start with a new model each time:\n",
    "#model = None\n",
    "model = SingleDigitMNISTNet().to(device=device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-5) \n",
    "#optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss() # our model outputs log_softmax(), i.e. we can use NLLLoss() here\n",
    "\n",
    "training(\n",
    "    epochs = 30,\n",
    "    train_loader = train_dl,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    optimizer = optimizer,\n",
    "    device = device,\n",
    "    show_progress = True,\n",
    "    L2_regularization = True,\n",
    "    L1_regularization = False,\n",
    ")\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.8504999876022339\n",
      "Accuracy val: 0.8540999889373779\n",
      "Validation finished.\n"
     ]
    }
   ],
   "source": [
    "validate(\n",
    "    model = model,\n",
    "    train_loader = train_dl,\n",
    "    val_loader = test_dl, \n",
    "    loss_fn = loss_fn\n",
    ")\n",
    "\n",
    "print(\"Validation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saveing the model to c:\\Users\\BoSc\\Documents\\9999_Public_Repository_Github\\BSC Public Github\\multi_digit_MNIST\\simplesaved_model_085val.pt.\n"
     ]
    }
   ],
   "source": [
    "# save, load the model via it's state-dict:\n",
    "\n",
    "import os\n",
    "\n",
    "#MODE=\"load\"\n",
    "MODE=\"save\"\n",
    "\n",
    "MODEL_PATH = os.path.join(os.getcwd(), \"simplesaved_model_085val.pt\")\n",
    "\n",
    "if( MODE==\"save\"):\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "if ( MODE == \"load\"):\n",
    "    #Load with this code:\n",
    "    model = SingleDigitMNISTNet(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "print(f\"Finished {MODE}ing the model to {MODEL_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saveing the model to c:\\Users\\BoSc\\Documents\\9999_Public_Repository_Github\\BSC Public Github\\multi_digit_MNIST\\torch_jit_model_085val.pt.\n"
     ]
    }
   ],
   "source": [
    "# save, load the model using TorchScript:\n",
    "# Using the TorchScript format, you will be able to load the exported model and run inference without defining the model class.\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices\n",
    "\n",
    "import os\n",
    "\n",
    "MODEL_PATH = os.path.join(os.getcwd(), \"torch_jit_model_085val.pt\")\n",
    "\n",
    "# chose load or save:\n",
    "#MODE=\"load\"\n",
    "MODE=\"save\"\n",
    "\n",
    "if( MODE==\"save\"):\n",
    "    model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "    model_scripted.save(MODEL_PATH) # Save\n",
    "\n",
    "if ( MODE == \"load\"):\n",
    "    model = torch.jit.load(MODEL_PATH)\n",
    "    model.eval() # call to prepare for inference - i.e. non-training\n",
    "\n",
    "print(f\"Finished {MODE}ing the model to {MODEL_PATH}.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e3b78e97e18c72126c3c5b3e515c7ce053012eba14c4d1ae01d8c98942b8394"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('conenv_multiMnist': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
